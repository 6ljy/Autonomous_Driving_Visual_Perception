# Autonomous_Driving_Visual_Perception
Autonomous driving visual perception project using YOLO model and OpenCV


该技术参考文档围绕基于 YOLO 模型的视觉场景理解项目展开，涵盖项目概述、目标、技术实践、汇报及参考资料等内容，旨在利用计算机视觉和深度学习实现视频实时检测分析，提升相关技术能力。

1. 项目概述：借助计算机视觉与深度学习技术，基于 YOLO 目标检测模型，开发低能耗、适应性强的视觉场景理解方案，应用于智能监控等领域，同时学习代码助手及 GitHub 项目管理技能。
2. 项目目标：设计并实现实时视觉场景理解视频处理系统，集成目标检测与追踪模型，完成多类别目标识别，搭建可视化界面，掌握 GitHub 管理技能，完成项目汇报。
3. 技术实践内容
    - 目标检测模型：介绍 YOLO 和 MediaPipe Object Detection 两种目标检测技术及对应文档链接。
    - 视频流处理与分析：运用 OpenCV 获取视频流，经预处理后，先以轻量级模型检测识别场景，再用复杂模型高精度识别、追踪目标。
    - 自适应视频处理：根据不同视觉场景，通过计算帧率、调整处理频率等方式，实现低功耗与实时性。
    - 数据可视化与界面开发：用 Matplotlib 等库可视化数据，基于 Streamlit 或 Flask 搭建 Web 界面，支持视频上传、实时流接入及参数调整。
    - 代码开发与管理：学习 Codeium 使用技巧，利用 Git 进行版本控制，遵循编码规范，编写项目文档并使用自动化工具更新。项目规划可借助 Trello 等工具跟踪任务。
4. 项目汇报
    - 指导原则：要求简洁明了、突出成果、可视化展示、逻辑清晰并针对不同听众调整重点。
    - PPT 设计：结构简洁、图文结合，准备文档记录技术细节。
    - 内容框架：包括引言、技术方案、系统实现、应用效果、挑战与解决、成果与计划、结论及 Q&A 环节。
5. 参考资料：提供目标检测、追踪、视频流处理、数据可视化、界面开发及代码管理等方面的学习链接。
